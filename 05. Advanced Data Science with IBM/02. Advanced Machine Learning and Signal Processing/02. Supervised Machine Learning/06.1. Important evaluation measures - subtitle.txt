It is important to know how good or how bad your algorithm is doing on your data. So, knowing how good or how bad you are is called evaluation. So, evaluation are measures of model prediction performance. In the case of classification, this is pretty simple and straightforward. So, here I will only cover accuracy which is the one which I mostly use. It's really straightforward. You just check on your dataset, how many of the data points you are predicting correctly, and how many you are not. Then you count those up, and then you take the correctly identified examples and divided by the number of overall examples, and then you're done. So in this case, I've classified four out of six examples correctly, that means it's four over six which gives us 66 percent of accuracy. In the case of regression, it's slightly more complicated. It's the accuracy measure in classification. I really like measures which are normalized, that means which gives us values between zero and one. One means good, zero means bad. This is the case for accuracy. Therefore in regression, I really like the R2 measure. So R2 is also called R squared and is calculated as follows. First of all, you take SS total. For calculating SS total, you just subtract the mean of your target variable from each individual target variable, you square it, and you sum it up. And you do the same thing for each predicted variable. So, FI in this case is the predicted variable. So, you subtract the mean from your predicted variable, you square it, and you sum all those up, and you get SS reg. Then, R squared is nothing else than one minus SS res or over SS total. So, if those measures you can assess the performance of your classifiers and your regression models. Accuracy you can use for classification, and R squared for regression. This should give you enough for now to understand how you can test the performance of your moderates.